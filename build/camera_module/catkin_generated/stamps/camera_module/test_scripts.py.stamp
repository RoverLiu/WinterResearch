#! /usr/bin/env python3

# from imutils.object_detection import non_max_suppression
from trolleybot_msgs.msg import HumanAngle
import numpy as np
import rospy
import copy
import cv2
from sensor_msgs.msg import CompressedImage
from numpy import asarray
import mediapipe as mp
import imutils
from imutils.object_detection import non_max_suppression

class human_detection:
    def __init__(self) -> None:
        # mediapipe init
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_pose = mp.solutions.pose
        self.pose = self.mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)
        # HOGdecriptor init
        self.HOGCV = cv2.HOGDescriptor()
        self.HOGCV.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())

        # camera details
        self.camera_width = 640
        self.camera_height = 480
        self.HFOV = 53.50
        self.VFOV = 41.6
        self.angle_offset = 0
        self.angle_boundary = 0
        self.count=0
        
        #topic subcribption 
        self.image_topic = 'raspicam_node/image/compressed_throttled'
        self.bag_file = '/home/rover/MTRX5700-Major-Project/bags/test2_scan.bag'
        self.human_angle_topic = "human_angle"
        
        self.is_lose = True
        
        rospy.init_node('human_detection', anonymous=True)
        rospy.Subscriber(self.image_topic, CompressedImage, self.cam_callback)
        self.pub = rospy.Publisher(self.human_angle_topic, HumanAngle, queue_size=10)
        rospy.spin()

    # camera callback function will publish a human relative angle 
    def cam_callback(self, ros_data):
        
        #image conversion 
        np_arr = np.fromstring(ros_data.data, np.uint8)
        image_np = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)

        cv2.imshow('cv_img', image_np)
        k = cv2.waitKey(10)
        #if not human detected, HOG-decriptor and HSV filter applies and no relative angle will be calculated
        if self.is_lose:
            print("Search for the person")         
            self.image_resize = imutils.resize(image_np, width = min(800, image_np.shape[1])) 
            result_image = self.detect_convention(self.image_resize)
            fileted_img = self.HSV_color_dection(result_image)
            if fileted_img is not None:
                self.is_lose = False
                angle = self.detect_pose(fileted_img, self.pose)
            else:
                res = HumanAngle()
                res.min_angle = 0
                res.max_angle = 0
                self.pub.publish(res)

        #Mediapipe applies while human detected 
        else:
            print("Keep tracking")
            angle = self.detect_pose(image_np, self.pose)
        
    def detect_pose(self, frame, pose):
        frame.flags.writeable = False
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        results = pose.process(frame)

        # Draw the pose annotation on the image.
        frame.flags.writeable = True
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
        self.mp_drawing.draw_landmarks(
            frame,
            results.pose_landmarks,
            self.mp_pose.POSE_CONNECTIONS,
            # landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())
        )
        # Flip the image horizontally for a selfie-view display.
        cv2.imshow('MediaPipe Pose', cv2.flip(frame, 1))
        cv2.waitKey(10)
        
        # calculate angle      
        if results.pose_landmarks:
            h2 = 0
            h1 = self.camera_width
            for landmark in results.pose_landmarks.landmark:  
                if (landmark.x < h1):
                    h1 = landmark.x
                if (landmark.x > h2):
                    h2 = landmark.x
                
            [angle_h1, angle_h2] = self.angle_update(h1*self.camera_width, h2*self.camera_width)
            print("Here is data for each - h1: {}, h2: {} angle {} {}".format(h1,h2, angle_h1, angle_h2))
            
            res = HumanAngle()
            res.min_angle = angle_h1/180*np.pi + self.angle_offset - self.angle_boundary
            res.max_angle = angle_h2/180*np.pi + self.angle_offset + self.angle_boundary
            self.pub.publish(res)
        else:
            self.is_lose = True
        
        return 0
    
    #HOG descriptor function
    def detect_convention(self,frame):
        bounding_box_cordinates, weights =  self.HOGCV.detectMultiScale(frame, winStride = (4, 4), padding = (8, 8), scale = 1.05)
        multi_image = np.zeros_like(frame)
        person = 1
        pick = non_max_suppression(bounding_box_cordinates, probs=None, overlapThresh=0.65)
        #draw the region of interest
        for x,y,w,h in pick:
            cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), 2)
            person += 1
            mask = np.zeros(frame.shape[:2], dtype="uint8")
            cv2.rectangle(mask, (x,y),(x+w,y+h), 255, -1)
            #extract all human related data 
            masked = cv2.bitwise_and(frame, frame, mask=mask)
            multi_image = cv2.bitwise_or(multi_image,masked)
        return multi_image
    
    #HSV color detection function 
    def HSV_color_dection(self,frame):
        lower_red = np.array([120, 60, 10])
        upper_red = np.array([179, 255, 130])
        # lower_red = np.array([0, 131, 49])
        # upper_red = np.array([179, 255, 255])
        hsv_image = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        red_mask = cv2.inRange(hsv_image, lower_red, upper_red)
        #if image has a HSV color
        s = sum(np.sum(red_mask, axis=0))
        if s > 1000000.0:
            red_image = cv2.bitwise_and(hsv_image, hsv_image, mask=red_mask)
            _, _, v = cv2.split(red_image)
            output_rgb = cv2.cvtColor(v, cv2.COLOR_GRAY2BGR)
            blur_image = cv2.GaussianBlur(output_rgb, (3, 3),
                                                cv2.BORDER_DEFAULT)
            gray = cv2.cvtColor(blur_image, cv2.COLOR_BGR2GRAY)
            edges = cv2.Canny(gray, 20, 200)
            contoured = copy.deepcopy(edges)
            cv2.waitKey(10)
            contours, _ = cv2.findContours(contoured, cv2.RETR_TREE,
                                                cv2.CHAIN_APPROX_SIMPLE)[-2:]
            #find ROI 
            if len(contours) > 0:
                for c in contours:
                    M = cv2.moments(c)
                    if M["m00"] != 0:
                            cX = int(M["m10"] / M["m00"])
                            cY = int(M["m01"] / M["m00"])
                    else:
                        cX, cY = 0, 0
                    ROI = max(contours, key=cv2.contourArea)
                    x, y, w, h = cv2.boundingRect(ROI)
                    #define the center of ROI 
                    self.center_x = x + w / 2
                    mask = np.zeros(frame.shape[:2], dtype="uint8")
                    cv2.rectangle(mask, (x-w,y-h),(x+2*w,y+2*h), 255, -1)
                    masked = cv2.bitwise_and(frame, frame, mask=mask)
                    cv2.imshow("HSV mask threshold", masked)
                    self.center_x_start = x
                    self.center_x_end =x+2*w
                    return masked
        return None

    def angle_update(self, h1, h2):
        # confirm whether only v1 and v2 are needed
        list_of_angles = []
        angle_h1 = float(h1-self.camera_width/2)/(self.camera_width/2)*(self.HFOV)
        angle_h2 = float(h2-self.camera_width/2)/(self.camera_width/2)*(self.HFOV)
        list_of_angles = [angle_h1,angle_h2]
        
        print(list_of_angles)
        return list_of_angles

if __name__ == "__main__":
    human_detection()
